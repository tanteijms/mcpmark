{
  "task_id": "file_cleanup_dedup_archive",
  "task_name": "Advanced File Cleanup, Dedup & Archive",
  "category_id": "file_management_v2",
  "category_name": "File Management",
  "description": "Clean up a shared team workspace with nested directories: recursively scan files, detect and remove duplicates by content comparison, validate JSON files and quarantine malformed ones, separate drafts, discard empty files, organize remaining files by type, and generate inventory, duplicates, and audit reports.",
  "author": "",
  "created_at": "2026-02-11",
  "difficulty": "L3",
  "tags": [
    "file organization",
    "deduplication",
    "json validation",
    "nested directory traversal",
    "content comparison",
    "batch processing",
    "report generation"
  ],
  "mcp": ["filesystem"],
  "meta_data": {
    "stateType": "text",
    "stateContent": "workspace/\n├── inbox/\n│   ├── Q1_Sales_Report.csv\n│   ├── q1_sales_backup.csv          (duplicate content)\n│   ├── meeting_notes_jan.txt\n│   ├── config_v2.json\n│   └── DRAFT_budget_proposal.txt\n├── downloads/\n│   ├── 2026-01/\n│   │   ├── server_metrics.csv\n│   │   ├── error_log.txt\n│   │   └── api_endpoints.json\n│   └── 2026-02/\n│       ├── Server Metrics.csv        (duplicate content)\n│       ├── analysis_results.json     (malformed JSON)\n│       ├── weekly_summary.txt\n│       └── DRAFT_review.txt\n├── shared/\n│   ├── team_contacts.csv\n│   ├── project_timeline.json\n│   ├── broken_export.json            (malformed JSON)\n│   ├── README.md\n│   └── old_scratch.txt               (empty)\n├── Sales_Data_Final.csv\n├── Backup_Config.JSON\n├── temp_notes.txt                    (empty)\n└── project_overview.md",
    "stateUrl": null,
    "stateOriginalUrl": null
  },
  "environment": {
    "platform": "cross-platform",
    "notes": "File sizes calculated as bytes/1024 (binary KB). Duplicate detection is content-based (byte-for-byte comparison). JSON validation checks syntactic correctness.",
    "python_version": ">=3.8"
  }
}
